{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_xlxhxFE7u_"
      },
      "source": [
        "## Brief Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KZ_UrAvD2mm"
      },
      "source": [
        "There are multiple types of generative models\n",
        "\n",
        "1. GANs\n",
        "2. VAEs\n",
        "3. Flow-based generative models\n",
        "    \n",
        "\n",
        "    \n",
        "    \n",
        "\n",
        "GANs and VAEs can perform pretty well in modeling the high dimensional distribution such as images. However, they have many limitations. Several of them are listed below:\n",
        "\n",
        "1. Neither of them allows us to compute the exact probability density of new data.\n",
        "2. Training of GANs can be challenge due to issues such as mode collapse, instability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e52oIQRFEkCu"
      },
      "source": [
        "### Normalizing Flows\n",
        "\n",
        "A Normalizing Flow (NF) is composed of a series of invertible transformation. In comparison with GANs and VAEs, NFs can not only perform generative modeling, but also do density estimation which has many applications in machine learning problems.\n",
        "\n",
        "Given samples from some distribution $p_X(x)$, NFs are trying to model it directly. And the essential idea behind NFs training is **Maximum Likelihood Estimation (MLE)**  \n",
        "\n",
        "$\\max_{\\theta\\in\\mathcal{M}} \\mathbb{E}_{x\\sim X}[\\log p_\\theta(x)]$\n",
        "\n",
        "Then a natural question would be: how to parameterize $p_{\\theta}(x)$ in practice?\n",
        "\n",
        "The fundamental idea is that if we can find the transformation between  $p_X(x)$ and some other distribution $p_Z(z)$ and we can somehow keep track of the \"change of density\" through that transformation, we could utilize the know closed-form of $p_Z(z)$ to compute $p_X(x)$. \n",
        "\n",
        "In specific, a NF is a invertible bijective function $f$ between distribution $p_X$ and $p_Z$. To help us model $p_X$, $p_Z$ should be some simple distribution whose density function is known in closed form. \n",
        "\n",
        "A preview of main requirements for designing a flow model:\n",
        "\n",
        "1. **$f$ must be invertible.** It allows us to transform between $x$ and $z$. In training, we are given $x$ and to compute the likelihood, we need to first transform $z = f(x)$. In generative modeling, we want to generate samples from $p_X(x)$, then we use the inverse transformation $x' = f^{-1}(z)$. How to achieve invertibility will be introduced in details when discussing different flow models.\n",
        "2. **The determinant of Jacobian of $f$ must be easy to compute**. The necessity of this will be explained in detail in the following section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRekYafIExMs"
      },
      "source": [
        "### Change of Variable Formula\n",
        "\n",
        "The key idea is that the density volume must be conserved (so that both $p_X$ and $p_Z$ will integrate to 1) i.e. $p_X(x)|dx|=p_Z(z)|dz|$. Then we can get the change of variable formula.\n",
        "\n",
        "$\\begin{aligned}\n",
        "p_{X}(x) &=p_{Z}(f(x))|\\operatorname{det}\\left(\\frac{\\partial f(x)}{\\partial x^{T}}\\right)| \\\\\n",
        "\\log \\left(p_{X}(x)\\right) &=\\log \\left(p_{Z}(f(x))\\right)+\\log \\left(|\\operatorname{det}\\left(\\frac{\\partial f(x)}{\\partial x^{T}}\\right)|\\right)\n",
        "\\end{aligned}$\n",
        "\n",
        "From the formula above, even if we don't know $p_X$, as long as we have access to $p_Z$ and can compute the Jacobian determinant of $f$, we can estimate $p_X$ with samples from it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uCqtYWoE6a8"
      },
      "source": [
        "### Different Flows\n",
        "There are many different ways of achieving invertibility and compute/approximate determinant of Jacobian. Here are \n",
        "\n",
        "1. Coupling Layers\n",
        "    1. RealNVP, Glow ...\n",
        "    2. Neural Spline Flows\n",
        "2. Autoregressive Flows\n",
        "    1. MAF, IAF \n",
        "3. Residual based \n",
        "    1. iResNet, Residual Flows\n",
        "4. ODE based \n",
        "    1. FFJORD, RNODE\n",
        "    \n",
        "\n",
        "We will explore **MAF** and **RealNVP** in this assignment.\n",
        "\n",
        "**Note:** There are multiple ways of implementing each kind of flow models. In this assignment, we just show one of them. Feel free to explore your own way of implementing flow mdoel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fv4EjBdoGTiK"
      },
      "source": [
        "## Exercise 1: RealNVP for 2D data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlhBvGKHJLp1"
      },
      "source": [
        "In this exercise, we will build a RealNVP for simple 2D data.\n",
        "\n",
        "The RealNVP is composed by stacking a sequence of affine coupling layers.\n",
        "\n",
        "$\\begin{aligned}\n",
        "\\mathbf{y}_{1: d} &=\\mathbf{x}_{1: d} \\\\\n",
        "\\mathbf{y}_{d+1: D} &=\\mathbf{x}_{d+1: D} \\odot \\exp \\left(s\\left(\\mathbf{x}_{1: d}\\right)\\right)+t\\left(\\mathbf{x}_{1: d}\\right)\n",
        "\\end{aligned}$\n",
        "\n",
        "where $\\mathbf{x}_{1: d}$ represents the first d component of the vector $\\mathbf{x}\\in \\mathbb{R}^D$. Note that $s\\left(\\mathbf{x}_{1: d}\\right)\\in \\mathbb{R}^{D-d}$. And $\\mathbb{s}$ and $\\mathbb{t}$ is usually parameterized by neural networks. $\\odot$ is the Hadamard product or element-wise product.\n",
        "\n",
        "It is called an affine coupling layer because it is in the form of $\\mathbf{y} =\\mathbf{A} \\mathbf{x} +\\mathbf{b}$ (affine) and part of the $\\mathbf{y}$ is the same as input while the other part of the $\\mathbf{y}$ is updated depending on itself and the remainder of it (coupling).\n",
        "\n",
        "Let's check the two important features mentioned above.\n",
        "\n",
        "**Invertibility**\n",
        "\n",
        "The inverse transformation is quite straightforward.\n",
        "\n",
        "$\\begin{aligned}\n",
        "\\mathbf{x}_{1: d} &=\\mathbf{y}_{1: d} \\\\\n",
        "\\mathbf{x}_{d+1: D} &=(\\mathbf{y}_{d+1: D}-t\\left(\\mathbf{x}_{1: d}\\right)) \\odot \\exp \\left(-s\\left(\\mathbf{x}_{1: d}\\right)\\right)\n",
        "\\end{aligned}$\n",
        "\n",
        "**Jacobian determinant**\n",
        "\n",
        "$\\mathbf{J}=\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}=\\left[\\begin{array}{cc}\n",
        "\\mathbb{I}_{d} & \\mathbf{0}_{d} \\\\\n",
        "\\frac{\\partial \\mathbf{y}{d+1: D}}{\\partial \\mathbf{x}{1: d}} & \\operatorname{diag}\\left(\\exp \\left(s\\left(\\mathbf{x}_{1: d}\\right)\\right)\\right)\n",
        "\\end{array}\\right]$\n",
        "\n",
        "The top right corner is $\\mathbf{0}_{d}$ because $\\mathbf{y}_{1: d}$ is independent of $\\mathbf{y}_{d+1: D}$. \n",
        "\n",
        "The bottom left corner can be complex but it does not matter.\n",
        "\n",
        "The bottom right is a diagonal matrix. The derivation is the same as why the top left is an identity matrix. The only difference is that each element of $\\mathbf{x}_{d+1: D}$ is scaled by $\\exp \\left(s\\left(\\mathbf{x}_{1: d}\\right)\\right)$.\n",
        "\n",
        "Then the determinant of Jacobian is just \n",
        "\n",
        "$\\operatorname{det}(\\mathbf{J})=\\exp \\left(\\sum_{j=1}^{D-d} s\\left(\\mathbf{x}_{1: d}\\right)_{j}\\right)$\n",
        "\n",
        "$\\operatorname{log}\\operatorname{det}(\\mathbf{J})=\\sum_{j=1}^{D-d} s\\left(\\mathbf{x}_{1: d}\\right)_{j}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkopIrmWjpir"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as F\n",
        "import torch.distributions as D\n",
        "import torchvision.transforms as T\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Y88KHefjpis"
      },
      "outputs": [],
      "source": [
        "def create_toy(tn, dataset):\n",
        "\n",
        "    # tn - controlling the number of samples\n",
        "    rng = 0\n",
        "\n",
        "    if dataset == 'moon': \n",
        "        X, y = datasets.make_moons(n_samples = int(200*tn) , noise = 0.05, random_state=rng)\n",
        "\n",
        "    elif dataset == 'checkerboard':\n",
        "        x1 = np.random.rand(int(200*tn)) * 4 - 2\n",
        "        x2_ = np.random.rand(int(200*tn)) - np.random.randint(0, 2, int(200*tn)) * 2\n",
        "        x2 = x2_ + (np.floor(x1) % 2)\n",
        "        X = np.concatenate([x1[:, None], x2[:, None]], 1) * 2\n",
        "        y = None\n",
        "\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiC0aRV1jy0P"
      },
      "source": [
        "Visualize the data before training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUAWceacjy0Q"
      },
      "outputs": [],
      "source": [
        "x,y = create_toy(tn=10, dataset='moon')\n",
        "fig = plt.figure(figsize = (4.8, 4.8))\n",
        "plt.scatter(*x.T,c=y,s=1)\n",
        "plt.title(\"training data\")\n",
        "plt.xlabel(r\"$x_1$\")\n",
        "plt.ylabel(r\"$x_2$\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgJNLOIXVJ7V"
      },
      "source": [
        "### Task 0: Implementation of $s$ and $t$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjmlInVpJmao"
      },
      "source": [
        "Recall that in the affine coupling layer, $s$ and $t$ is typically parameterized by neural network. We define a simple Multilayer Perceptron (MLP) for you below to use for $s$ and $t$. [wiki_mlp](https://en.wikipedia.org/wiki/Multilayer_perceptron) You must run this cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mDDS1j9Jgu1"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "\n",
        "    # Multi Layer Perceptron to parameterize s and t\n",
        "    def __init__(self, n_hidden, hidden_dim, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Define the layers and activation functions that will be used in forward function\n",
        "        layers = []\n",
        "        layers.append(nn.Linear(input_dim,hidden_dim))\n",
        "        layers.append(nn.ReLU())\n",
        "        for _ in range(n_hidden - 1):\n",
        "            layers.append(nn.Linear(hidden_dim,hidden_dim))\n",
        "            layers.append(nn.ReLU())\n",
        "        layers.append(nn.Linear(hidden_dim,output_dim))\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Transform the input x \n",
        "        # expected input shape - [N, input_dim] where N is the number of samples\n",
        "        # expected output shape - [N, output_dim]\n",
        "        \n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtakyVUXV5JI"
      },
      "source": [
        "### Task 1: Implementation of affine coupling layer (30/100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQOt5OLYWAZu"
      },
      "source": [
        "In this task, you should implement the affine coupling layer which is the basic building block of RealNVP. \n",
        "\n",
        "Additionally, we can use a binary mask $b$ to implement partitioning (i.e., which variables are fixed and which are transformed). Note that the binary mask should be the same shape as one sample.\n",
        "This is an implementation choice and is not technically required by the theoretic structure of RealNVP.\n",
        "In particular, $b \\odot x$ extracts the \"fixed\" coordinates of $x$ and makes others 0.  $(1-b) \\odot x$ would extract the \"transformed\" coordinates of $x$ and make others 0. Also, note that adding these two together gives the original input, i.e., $b \\odot x + (1-b) \\odot x \\equiv x$.\n",
        "One nice property of using masks is that the vectors are always the same dimension, just some coordinates are 0s.\n",
        "\n",
        "Given this use of the binary mask $b$, we can write the affine coupling layer formula as follows in a single equation:\n",
        "\n",
        "$\\begin{aligned}\n",
        "z &= b \\odot x + (1 - b) \\odot (x \\odot \\exp(s(b \\odot x)) + t(b \\odot x)).\n",
        "\\end{aligned}$\n",
        "\n",
        "The first part $b \\odot x$ extracts the fixed dimensions, and the $(1-b) \\odot$ part is dealing with the transformed dimensions. Note also that the input to $s$ and $t$ is $b \\odot x$, i.e., the fixed dimensions + 0s for other dimensions.\n",
        "\n",
        "And the inverse function can be written as:\n",
        "\n",
        "$\\begin{aligned}\n",
        "x &= b \\odot z + (1 - b) \\odot ((z - t(b \\odot z)) \\odot \\exp(-s(b \\odot z))).\n",
        "\\end{aligned}$\n",
        "\n",
        "Notice that $b \\odot z \\equiv b \\odot x$ because these features do not change.\n",
        "\n",
        "To help with training, it was found that $s$ should have the following form:\n",
        "\n",
        "$\\begin{aligned}\n",
        "s(x \\odot b) &= \\alpha \\cdot ReLU(f(x \\odot b)) + \\beta,\n",
        "\\end{aligned}$\n",
        "\n",
        "where $f(\\cdot)$ is an MLP, $\\alpha$ is the scale coefficient, and $\\beta$ is the shift coefficient. At the forward pass, $x$ represents the input data, and at the inverse pass, $x$ is the input feature.\n",
        "\n",
        "The $t$ function can simply be an MLP.\n",
        "\n",
        "Based on the formulas above, fill in the code below where indicated. The comments will tell you what is needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pf3T7wbul4WM",
        "outputId": "300827b2-0e77-418b-f74b-5ff76e17056a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Passed the invertibility check 2.32831e-07\n"
          ]
        }
      ],
      "source": [
        "class AffineCoupling(nn.Module):\n",
        "\n",
        "    def __init__(self, mask, n_hidden, hidden_dim, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mask = nn.Parameter(mask, requires_grad = False)\n",
        "\n",
        "        # two learnable parameters that help the training of the model\n",
        "        self.alpha = nn.Parameter(torch.zeros(1), requires_grad=True)\n",
        "        self.beta = nn.Parameter(torch.zeros(1), requires_grad=True)\n",
        "\n",
        "        # create two MLPs that will be used to parameterized s_net and t_net\n",
        "        ###########################   <YOUR CODE>  ############################\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ###########################   <YOUR CODE>  ############################\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Recommended implementation (self.mask will determine which features \n",
        "        #  are fixed and which are transformed)\n",
        "        # Step 1: Compute s and t from fixed features.\n",
        "        # Step 2: Compute transformed features based on outputs of s and t.\n",
        "        # Step 3: Compute the log determinant of Jacobian\n",
        "\n",
        "        ###########################   <YOUR CODE>  ############################\n",
        "\n",
        "\n",
        "        \n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ###########################   <YOUR CODE>  ############################\n",
        "\n",
        "        return z, logdet\n",
        "    \n",
        "    def inverse(self, z):\n",
        "\n",
        "        # Recommended implementation\n",
        "        # Step 1: Compute s and t.\n",
        "        # Step 2: Compute output using inverse of f.\n",
        "        # (No log determinant needed for inverse)\n",
        "\n",
        "        ###########################   <YOUR CODE>  ############################\n",
        "\n",
        "        \n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "        ###########################   <YOUR CODE>  ############################\n",
        "        return x\n",
        "\n",
        "\n",
        "# Check of the implementation\n",
        "# Suppose the input is $x$, the forward process is $f$, the backward process is $f^{-1}$. We expect $x=f^{-1}(f(x))$.\n",
        "def check_invertibility(model):\n",
        "  x_input = torch.randn((100, 2))\n",
        "\n",
        "  z, _ = model(x_input)\n",
        "  inverse_x = model.inverse(z)\n",
        "\n",
        "  difference = abs((inverse_x - x_input).sum())\n",
        "  print(f\"Passed the invertibility check {difference:g}\" if difference < 1e-5 else \"Did Not Pass, the difference {:f} is large\".format(difference))\n",
        "\n",
        "check_invertibility(AffineCoupling(nn.Parameter(torch.Tensor([1.0, 0.0]), requires_grad = False), 1, 16, 2, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Pa5fdw8W_0X"
      },
      "source": [
        "### Task 2: Implementation of RealNVP (30/100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHOvK-xhW_aq"
      },
      "source": [
        "Now define the final RealNVP model by stacking affine coupling layers you just defined.\n",
        "\n",
        "In practice, only using mask $[0,1]$ or $[1,0]$ will limit the expressiveness of the model. A widely used implementation is to stack affine coupling layers with mask alternating between $[0,1]$ and $[1,0]$.\n",
        "\n",
        "The log probability of $x$ is:\n",
        "\n",
        "$\\begin{aligned}\n",
        "\\log (p_{X}(x)) &= \\log \\left(p_{Z}(f(x)) \\right) + \\log \\det (\\mathbf{J}),\n",
        "\\end{aligned}$\n",
        "\n",
        "which can be used to calculated mean negative log-likelihood as the object of optimation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOPR-7N6mtlx",
        "outputId": "35ea8f3b-d80f-4340-a638-6c2339086a06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Passed the invertibility check 1.14553e-07\n"
          ]
        }
      ],
      "source": [
        "class NVP2D(nn.Module):\n",
        "\n",
        "    def __init__(self, masks, n_hidden, hidden_dim, input_dim=2, output_dim=2):\n",
        "        super().__init__()\n",
        "\n",
        "        # the masks are expected to be a list containing masks [0,1] and [1,0] \n",
        "        # An example could be [[0,1],[1,0],[0,1],[1,0],...]\n",
        "        self.masks = nn.ParameterList(\n",
        "            [nn.Parameter(torch.Tensor(m), requires_grad = False)\n",
        "            for m in masks]\n",
        "        )\n",
        "\n",
        "        # Feel free to change it if you define the AffineCoupling Layers in a different way\n",
        "        # The number of affine coupling layers would be equal to the number of elements in the masks list\n",
        "        self.flows = nn.ModuleList(\n",
        "            [AffineCoupling(mask = mask, n_hidden=n_hidden, hidden_dim=hidden_dim, input_dim=input_dim, output_dim=input_dim)\n",
        "            for mask in self.masks\n",
        "             ]\n",
        "        )\n",
        "        self.register_buffer('base_dist_mean', torch.zeros(input_dim))\n",
        "        self.register_buffer('base_dist_var', torch.ones(input_dim))\n",
        "        self.base_dist = D.Normal(self.base_dist_mean, self.base_dist_var)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Return both the transformed z and the total logdet for each sample\n",
        "        ###########################   <YOUR CODE>  ############################\n",
        "        \n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ###########################   <YOUR CODE>  ############################\n",
        "        return z, logdet\n",
        "\n",
        "    def inverse(self, z):\n",
        "        \n",
        "        # model the transformation x = f^-1(z)\n",
        "        ###########################   <YOUR CODE>  ############################\n",
        "        \n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ###########################   <YOUR CODE>  ############################    \n",
        "        return x\n",
        "\n",
        "    def log_prob(self, x):\n",
        "        # propagate through the flow to get z and logdet\n",
        "        # compute log likelihood log(p(x)) using the the change of variable formula\n",
        "        # self.base_dist.log_prob(z) can be used to compute the log likelihood of Gaussian \n",
        "        # after computing the likelihood at each feature, you can sum over the second axis\n",
        "        # the output shape should be [n_samples,]\n",
        "        ###########################   <YOUR CODE>  ############################\n",
        "        \n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "        ###########################   <YOUR CODE>  ############################\n",
        "        \n",
        "# Check invertibility\n",
        "masks = [[1.0, 0.0],\n",
        "         [0.0, 1.0],\n",
        "         [1.0, 0.0],         \n",
        "         [0.0, 1.0],]\n",
        "n_hidden = 2\n",
        "hidden_dim = 256\n",
        "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model = NVP2D(masks, n_hidden, hidden_dim)\n",
        "check_invertibility(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dft0pd58Xb9X"
      },
      "source": [
        "### Task 3: Training the model (20/100)\n",
        "\n",
        "Now you should train the RealNVP model you just define. In specific, at each iteration, create new samples from the toy distribution (it is like sampling a batch of data from a dataset with infinite data). Then compute the loss using the mean of log likelihood returned by the model.\n",
        "\n",
        "We have provided some basic setup here.  You should run it for at least 4000 iterations as in the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXZnR9IPmxEi"
      },
      "outputs": [],
      "source": [
        "# Define the masks, model and optimizers as needed\n",
        "# feel free to change it!\n",
        "\n",
        "masks = [[1.0, 0.0],\n",
        "         [0.0, 1.0],\n",
        "         [1.0, 0.0],         \n",
        "         [0.0, 1.0],]\n",
        "n_hidden = 2\n",
        "hidden_dim = 256\n",
        "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model = NVP2D(masks, n_hidden, hidden_dim)\n",
        "model = model.to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr = 1e-5, weight_decay=1e-5)\n",
        "n_iters = 4000 \n",
        "\n",
        "for idx in range(n_iters):\n",
        "    # At each iteration, first create training data using the given function create_toy (note y is not useful here)\n",
        "    # Set tn = 1\n",
        "    ###########################   <YOUR CODE>  ############################\n",
        "    ## sample data from the scipy moon dataset\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "    ## calculate the negative loglikelihood of X\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    ###########################   <YOUR CODE>  ############################\n",
        "    if (idx + 1) % 100 == 0:\n",
        "        print(f\"At iteration: {idx:}, loss: {loss.item():.5f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpHWPHGOZOuR"
      },
      "source": [
        "### Task 4: Evaluation (20/100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "885pY1__vX-p"
      },
      "source": [
        "Implement three simple functions (only one line is required for each): \n",
        "\n",
        "1. One function to transform the data to the latent space\n",
        "2. One function to map the latent back to the data space\n",
        "3. One function to compute the likelihood value (note NOT the log likelihood but the actual likelihood for plotting the density)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4t5SxS7-vtaF"
      },
      "outputs": [],
      "source": [
        "def data2latent(x):\n",
        "  model.cpu()\n",
        "  ###########################   <YOUR CODE>  ############################\n",
        "  \n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "  ###########################   <YOUR CODE>  ############################\n",
        "  return z\n",
        "\n",
        "def latent2data(z):\n",
        "  model.cpu()\n",
        "  ###########################   <YOUR CODE>  ############################\n",
        "  \n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "  ###########################   <YOUR CODE>  ############################\n",
        "  return x\n",
        "\n",
        "def get_likelihood(x):\n",
        "  model.cpu()\n",
        "  ###########################   <YOUR CODE>  ############################\n",
        "  \n",
        "  \n",
        "\n",
        "\n",
        "  \n",
        "  ###########################   <YOUR CODE>  ############################\n",
        "  return l"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzT5W62eZXeS"
      },
      "source": [
        "The following code will evaluate your trained model and your functions above. For simplicity, we have implemented these parts to help you focus on the fundamentals of normalizing flows.\n",
        "\n",
        "**Part 1**: Transform the data to the latent space.  If the model fits the data distribution perfectly, this should look like a standard normal distribution. In general, the model will not fit the data distribution perfectly, so it will not look like exactly like a normal distribution but should have the mass concentrated near the middle.\n",
        "\n",
        "**Part 2**: Generate new \"fake\" data.  First sample from the normal distribution and then pass through the inverse of your flow model.  This is like the reverse of the previous step. If the model is perfect (which it never is), the generated data will look like the original toy data.\n",
        "\n",
        "**Part 3**: Density estimation.\n",
        "First, create a density heat map using the `xline` and `yline` given (which are tensors representing the x, y coordinate). In specific, you should compute the likelihood at each point correponding to those coordinates.\n",
        "\n",
        "You can use [torch.meshgrid](https://pytorch.org/docs/stable/generated/torch.meshgrid.html) to make the meshgrid, and [contourf](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.contourf.html) to plot the contour.\n",
        "\n",
        "Second, test the outliers. Choose 11 points from -1 to 1, i.e. -1.0, -0.8, -0.6, ... , 0.8, 1.0. Then compute the likelihood (not log likelihood) and decide whether it is an outlier by checking whether $p(x)<0.01$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvaAF98pnVhs"
      },
      "outputs": [],
      "source": [
        "# test if f can transform from x to z\n",
        "# create two scatter plots, the first is the original test data x as provided\n",
        "# the second is the generated z\n",
        "# visualize the class label in the scatter plot as given in the previous example\n",
        "# for z, you should expect seeing something like a standard normal distribution\n",
        "\n",
        "x_test,y_test = create_toy(tn=10, dataset='moon')\n",
        "x = torch.Tensor(x_test).cpu()\n",
        "\n",
        "z = data2latent(x)\n",
        "\n",
        "z = z.cpu().detach().numpy()\n",
        "\n",
        "fig = plt.figure(2, figsize = (12.8, 10.8))\n",
        "fig.clf()\n",
        "plt.subplot(2,3,1)\n",
        "plt.scatter(*x.cpu().T,c=y_test,s=1)\n",
        "plt.title(\"test data\")\n",
        "plt.xlabel(r\"$x_1$\")\n",
        "plt.ylabel(r\"$x_2$\")\n",
        "\n",
        "plt.subplot(2,3,2)\n",
        "plt.scatter(*z.T, c=y_test, s=1)\n",
        "plt.title(\"z transformed from x\")\n",
        "plt.xlabel(r\"$z_1$\")\n",
        "plt.ylabel(r\"$z_2$\")\n",
        "\n",
        "\n",
        "# test if inverse of f can transform from z to x\n",
        "# create two scatter plots, the first is sampled from the standard gaussian distribution as provided\n",
        "# the second is the generated x transformed from z via inverse of f\n",
        "# visualize the class label in the scatter plot\n",
        "# from new data, you should expect to see something like the original moon\n",
        "\n",
        "z = torch.normal(0, 1, size = (1000, 2)).cpu()\n",
        "\n",
        "X = latent2data(z)\n",
        "\n",
        "X = X.cpu().detach().numpy()\n",
        "z = z.cpu().detach().numpy()\n",
        "\n",
        "\n",
        "plt.subplot(2,3,4)\n",
        "plt.scatter(*z.T,s=1)\n",
        "plt.title(\"Z sampled from normal distribution\")\n",
        "plt.xlabel(r\"$z_1$\")\n",
        "plt.ylabel(r\"$z_2$\")\n",
        "\n",
        "plt.subplot(2,3,5)\n",
        "plt.scatter(*X.T,s=1)\n",
        "plt.title(\"X transformed from Z\")\n",
        "plt.xlabel(r\"$x_1$\")\n",
        "plt.ylabel(r\"$x_2$\")\n",
        "\n",
        "\n",
        "# create the density heat map using log likelihood on a meshgrid generated by 100 points\n",
        "# from -1.5 to 2.5 in both dimension (10000 points in total)\n",
        "xline = torch.linspace(-1.5,2.5,100)\n",
        "yline = torch.linspace(-1.5,2.5,100)\n",
        "xgrid, ygrid = torch.meshgrid(xline, yline)\n",
        "xyinput = torch.cat([xgrid.reshape(-1, 1), ygrid.reshape(-1, 1)], dim=1)\n",
        "\n",
        "with torch.no_grad():\n",
        "    zgrid = get_likelihood(xyinput).reshape(100, 100)\n",
        "\n",
        "plt.subplot(2,3,3)\n",
        "plt.contourf(xgrid.cpu().numpy(), ygrid.cpu().numpy(), zgrid.cpu().numpy())\n",
        "plt.title('Estimated Density with outliers/inliers')\n",
        "\n",
        "#---------------------------------------------------------------------#\n",
        "xcor = torch.linspace(-1,1,11).view(-1,1)\n",
        "ycor = torch.zeros_like(xcor).view(-1,1)\n",
        "x = torch.cat((xcor,ycor),dim=-1)\n",
        "\n",
        "# compute the likelihood at each point and decide whether it is an outlier\n",
        "prob = get_likelihood(x)\n",
        "plt.plot(*x[prob<0.01].T, 'rx')\n",
        "plt.plot(*x[prob>=0.01].T, 'yo')\n",
        "plt.show()\n",
        "\n",
        "for i in range(x.shape[0]):\n",
        "    print('at ({:.1f},{}), the likelihood is {:.10f} and whether it is an outlier? - {}'.format(x[i,0].item(),x[i,1].item(),prob[i].item(), prob[i]<0.01))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyAPtUfsB44T"
      },
      "source": [
        "## (Optional, ungraded) Exercise 2: MAF for 2D data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYL1K01QhYAj"
      },
      "source": [
        "\n",
        "An autoregressive model predicts future behavior on past behavior and can be used to predict the sequential data. For example, given sequential data $\\mathbf{x} = [x_1, \\cdots, x_D]$, the joint distribution will be modeled as below\n",
        "\n",
        "$p(\\mathbf{x})=\\prod_{i=1}^{D} p\\left(x_{i} \\mid x_{1}, \\ldots, x_{i-1}\\right)=\\prod_{i=1}^{D} p\\left(x_{i} \\mid x_{1: i-1}\\right)$\n",
        "\n",
        "An autoregressive flow is a flow model that is composed as a autoregressive model.\n",
        "\n",
        "MAF represents Masked Autoregressive Flow. Given known distributions $p(z)$, MAF models $p(x)$ assuming that $x_i$ is dependent on $x_{1:i-1}$.\n",
        "\n",
        "In specific (the notation follows from the paper [1705.07057.pdf (arxiv.org)](https://arxiv.org/pdf/1705.07057.pdf)), \n",
        "\n",
        "$\\mathbf{x}_{i}=\\mathbf{z}_{i} \\odot \\exp(\\alpha_{i}\\left(\\mathbf{x}_{1: i-1}\\right))+\\mu_{i}\\left(\\mathbf{x}_{1: i-1}\\right)$\n",
        "\n",
        "where $\\sigma_i$ and $\\mu_i$ are parameterized by neural networks. In MAF, they are typically parameterized by Masked Autoencoder for Distribution Estimation (MADE).\n",
        "Figure 1 in the paper ([MADE: Masked Autoencoder for Distribution Estimation (arxiv.org)](https://arxiv.org/pdf/1502.03509.pdf)) is a great example and will be referred to below.\n",
        "The key to MADE is that the inputs can only affect \"downstream\" outputs of this neural network, which ensures the autoregressive property that is required, e.g., $x_5$ cannot affect the 3rd output $\\alpha_3$ but it can affect the 7th output $\\alpha_7$. More formally, this means $\\alpha_i = f_i(x_1, x_2, \\cdots, x_{i-1})$ and similarly for $\\mu_i$.\n",
        "\n",
        "The left network computes as (notation from paper [MADE: Masked Autoencoder for Distribution Estimation (arxiv.org)](https://arxiv.org/pdf/1502.03509.pdf))\n",
        "\n",
        "$\\begin{aligned}\n",
        "\\mathbf{h}^{l} &=\\operatorname{activation}^{l}\\left(\\mathbf{W}^{l} \\mathbf{h}^{l-1}+\\mathbf{b}^{l}\\right) \\\\\n",
        "\\hat{\\mathbf{x}} &=\\sigma\\left(\\mathbf{V h}^{L}+\\mathbf{c}\\right)\n",
        "\\end{aligned}$\n",
        "\n",
        "The right network computes as \n",
        "\n",
        "$\\begin{aligned}\n",
        "\\mathbf{h}^{l} &=\\text { activation }^{l}\\left(\\left(\\mathbf{W}^{l} \\odot \\mathbf{M}^{\\mathbf{W}^{l}}\\right) \\mathbf{h}^{l-1}+\\mathbf{b}^{l}\\right) \\\\\n",
        "\\hat{\\mathbf{x}} &=\\sigma\\left(\\left(\\mathbf{V} \\odot \\mathbf{M}^{V}\\right) \\mathbf{h}^{L}+\\mathbf{c}\\right)\n",
        "\\end{aligned}$\n",
        "\n",
        "Each node is randomly assigned an integer between $1$ and $D-1$ represented as $m_k^l$. For example, in the plot above, the 3rd node(k=3) at the 1st layer(l=1) $m_3^1=2$. The binary mask $\\mathbf{M}$ is defined as below\n",
        "\n",
        "$\\begin{aligned}\n",
        "&M_{k^{\\prime}, k}^{\\mathbf{W}^{l}}=\\mathbf{1}_{m_{k^{\\prime}}^{l} \\geq m_{k}^{l-1}}= \\begin{cases}1, & \\text { if } m_{k^{\\prime}}^{l} \\geq m_{k}^{l-1} \\\\\n",
        "0, & \\text { otherwise }\\end{cases} \\\\\n",
        "&M_{d, k}^{\\mathbf{V}}=\\mathbf{1}_{d \\geq m_{k}^{L}}= \\begin{cases}1, & \\text { if } d>m_{k}^{L} \\\\\n",
        "0, & \\text { otherwise }\\end{cases}\n",
        "\\end{aligned}$\n",
        "\n",
        "The weight is non-zero only if the integer assigned to its corresponding unit is larger than the the integer assigned to its previous unit. To make sure that all hidden units are connected to at least one input and output, we should make sure $m_k^l \\geq \\min_{k^{\\prime}} m_{k^{\\prime}}^{l-1}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8RjVDSGBnsA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as F\n",
        "import torch.distributions as D\n",
        "import torchvision.transforms as T\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rt5cZ8ZRB7HO"
      },
      "outputs": [],
      "source": [
        "def create_toy(tn, dataset):\n",
        "\n",
        "    # tn - controlling the number of samples\n",
        "    rng = 0\n",
        "\n",
        "    if dataset == 'moon': \n",
        "        X, y = datasets.make_moons(n_samples = int(200*tn) , noise = 0.05, random_state=rng)\n",
        "\n",
        "    elif dataset == 'checkerboard':\n",
        "        x1 = np.random.rand(int(200*tn)) * 4 - 2\n",
        "        x2_ = np.random.rand(int(200*tn)) - np.random.randint(0, 2, int(200*tn)) * 2\n",
        "        x2 = x2_ + (np.floor(x1) % 2)\n",
        "        X = np.concatenate([x1[:, None], x2[:, None]], 1) * 2\n",
        "        y = None\n",
        "\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qw8qB3hukMMr"
      },
      "source": [
        "### Task 1: Implementation of MADE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAYtRGJakviu"
      },
      "source": [
        "The code for creating masks and masked linear has already been provided.\n",
        "\n",
        "**Note:** MADE is not an invertible flow model. For simplicity, we implement it as an invertible model by incorporating it with affine transformation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wh9T1e5B-Ph"
      },
      "outputs": [],
      "source": [
        "def create_masks(input_size, hidden_size, n_hidden, input_degrees=None):\n",
        "\n",
        "    # first assign degrees to each node \n",
        "    # create mask i.e. whether the corresponding w in the linear layers will be used\n",
        "    # the mask should be a list of tensors that is in the same shape of weight matrix in the linear layer to be used\n",
        "    # return the mask list and the tensor corresponding to the degrees of input nodes (in the shape of [input_dim,])\n",
        "    degrees = []\n",
        "\n",
        "    degrees += [torch.arange(input_size)] if input_degrees is None else [input_degrees]\n",
        "    for _ in range(n_hidden + 1):\n",
        "        degrees += [torch.arange(hidden_size) % (input_size - 1)]\n",
        "    degrees += [torch.arange(input_size) % input_size - 1] if input_degrees is None else [input_degrees % input_size - 1]\n",
        "\n",
        "    # construct masks\n",
        "    masks = []\n",
        "    for (d0, d1) in zip(degrees[:-1], degrees[1:]):\n",
        "        masks += [(d1.unsqueeze(-1) >= d0.unsqueeze(0)).float()]\n",
        "\n",
        "    return masks, degrees[0]\n",
        "\n",
        "class MaskedLinear(nn.Linear):\n",
        "    #MADE building block layer\n",
        "    \n",
        "    def __init__(self, input_size, n_outputs, mask):\n",
        "        super().__init__(input_size, n_outputs)\n",
        "\n",
        "        self.register_buffer('mask', mask)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.linear(x, self.weight * self.mask, self.bias)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUKzRsIiCNW6"
      },
      "outputs": [],
      "source": [
        "class MADE(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, n_hidden, input_degrees=None):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # create the masks \n",
        "        # remember to swap the degree of the input nodes by assigning input_degrees\n",
        "        ###########################   <YOUR CODE>  ############################\n",
        "        \n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "        ###########################   <YOUR CODE>  ############################\n",
        "\n",
        "        # construct the model using a few MaskedLinear layers\n",
        "        # the basic structure would be same as constructing a fully connected layer using nn.Linear and nn.Sequential\n",
        "        # remember to add activation function such as Tanh() after each layer except the last layer\n",
        "        self.net_input = MaskedLinear(input_size, hidden_size, masks[0])\n",
        "        activation_fn = nn.Tanh()\n",
        "        self.net = []\n",
        "        for m in masks[1:-1]:\n",
        "            self.net += [activation_fn, MaskedLinear(hidden_size, hidden_size, m)]\n",
        "        self.net += [activation_fn, MaskedLinear(hidden_size, 2 * input_size, masks[-1].repeat(2,1))]\n",
        "        self.net = nn.Sequential(*self.net)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # first generate mu and alpha using the network you define above\n",
        "        # compute z using z = (x-mu) * exp(-alpha)\n",
        "        # compute the log determinant of Jacobian\n",
        "        # return z and log determinant of Jacobian\n",
        "        ###########################   <YOUR CODE>  ############################\n",
        "        \n",
        "        \n",
        "\n",
        "\n",
        "        \n",
        "        ###########################   <YOUR CODE>  ############################\n",
        "        return z, logdet\n",
        "\n",
        "    def inverse(self, z):\n",
        "        # inverse propagation\n",
        "        x = torch.zeros_like(z)\n",
        "        for i in self.input_degrees:\n",
        "            mu, loga = self.net(self.net_input(x)).chunk(chunks=2, dim=1)\n",
        "            x[:,i] = z[:,i] * torch.exp(loga[:,i]) + mu[:,i]\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXnAE5gatC6j"
      },
      "source": [
        "### Task 2: Implementation of MAF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyWnu_DzWrka"
      },
      "source": [
        "Define the MAF model.\n",
        "\n",
        "In this task, while stacking MADE to construct MAF, alternate the degree of two input dimension. In specific, set them to be $[0,1] $ and $[1,0]$ alternatively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfUdR04KCQuw"
      },
      "outputs": [],
      "source": [
        "\n",
        "class MAF2D(nn.Module):\n",
        "    def __init__(self, n_blocks, input_size, hidden_size, n_hidden, flip=True):\n",
        "        super().__init__()\n",
        "        \"\"\"\n",
        "        n_blocks - number of MADE \n",
        "        input_size - input dimension\n",
        "        hidden_size - hidden dimension of MADE\n",
        "        n_hidden - number of hidden layers in each MADE\n",
        "        flip - whether to flip the degree of input dimension in each MADE\n",
        "        \"\"\"\n",
        "        # base distribution for calculation of log prob under the model\n",
        "        # can be used by calling self.base_dist.log_prob(z)\n",
        "        self.register_buffer('base_dist_mean', torch.zeros(input_size))\n",
        "        self.register_buffer('base_dist_var', torch.ones(input_size))\n",
        "        self.base_dist = D.Normal(self.base_dist_mean, self.base_dist_var)\n",
        "\n",
        "        # construct model\n",
        "        flows = []\n",
        "        \n",
        "        for i in range(n_blocks):\n",
        "            \n",
        "            if flip:\n",
        "                if i%2 == 0:\n",
        "                    input_degrees = torch.LongTensor([0,1])\n",
        "                else: \n",
        "                    input_degrees = torch.LongTensor([1,0])\n",
        "            else:\n",
        "                input_degrees = None\n",
        "\n",
        "            flows += [MADE(input_size, hidden_size, n_hidden, input_degrees = input_degrees)]\n",
        "\n",
        "        self.flows = nn.ModuleList(flows)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # model the transformation z = f(x)\n",
        "        # propagate through the flow defined above\n",
        "        # remember to keep track of the log Jacobian determinant \n",
        "        ###########################   <YOUR CODE>  ############################\n",
        "        \n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ###########################   <YOUR CODE>  ############################\n",
        "\n",
        "    def inverse(self, z):\n",
        "        # model the transformation x = f^{-1}(z)\n",
        "        # propagate through the inverse of the flow\n",
        "        ###########################   <YOUR CODE>  ############################\n",
        "        \n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ###########################   <YOUR CODE>  ############################\n",
        "\n",
        "    def log_prob(self, x):\n",
        "        # propagate through the flow to get z\n",
        "        # compute log likelihood p(x) using the the change of variable formula\n",
        "        # self.base_dist.log_prob(z) can be used to compute the log likelihood of Gaussian \n",
        "        ###########################   <YOUR CODE>  ############################\n",
        "        \n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "        ###########################   <YOUR CODE>  ############################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbp7ifnJtLDY"
      },
      "source": [
        "### Task 3: Training the model\n",
        "Follow the same step as Exericise 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zD2yJ2uQZZ4h"
      },
      "outputs": [],
      "source": [
        "x,y = create_toy(tn=10, dataset='moon')\n",
        "fig = plt.figure(figsize = (4.8, 4.8))\n",
        "plt.scatter(*x.T,c=y,s=1)\n",
        "plt.title(\"training data\")\n",
        "plt.xlabel(r\"$x_1$\")\n",
        "plt.ylabel(r\"$x_2$\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpDOeUhVZS3t"
      },
      "source": [
        "Training and testing using **moon** data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEOmdUdbCQ4Q"
      },
      "outputs": [],
      "source": [
        "# define the MAF model\n",
        "# define the optimizer and n_iters as needed \n",
        "# feel free to change it!\n",
        "\n",
        "n_blocks = 8\n",
        "hidden_dim = 48\n",
        "input_size = 2\n",
        "n_hidden = 2\n",
        "\n",
        "model = MAF2D(n_blocks=n_blocks, input_size=2, hidden_size=hidden_dim, n_hidden=n_hidden)\n",
        "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model = model.to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr = 1e-4, weight_decay=1e-5)\n",
        "n_iters = 2000\n",
        "\n",
        "\n",
        "for idx in range(n_iters):\n",
        "\n",
        "    # generative new 200 samples at each iteration using create_toy with tn=1\n",
        "    # compute the loss and backpropagate\n",
        "    # print out the loss after each 100 iterations\n",
        "    ###########################   <YOUR CODE>  ############################\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    ###########################   <YOUR CODE>  ############################\n",
        "    if (idx + 1) % 100 == 0:\n",
        "        print(f\"idx_steps: {idx:}, loss: {loss.item():.5f}\")\n",
        "        \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBguM62huSeQ"
      },
      "source": [
        "### Task 4: Evaluation.\n",
        "Follow the same three steps as Exercise 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvSx0HIyCTJu"
      },
      "outputs": [],
      "source": [
        "# test if f can transform from x to z\n",
        "# create two scatter plots, the first is the original test data x\n",
        "# the second is the generated z\n",
        "# visualize the class label in the scatter plot\n",
        "\n",
        "x_test,y_test = create_toy(tn=10, dataset='moon')\n",
        "###########################   <YOUR CODE>  ############################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###########################   <YOUR CODE>  ############################\n",
        "\n",
        "fig = plt.figure(2, figsize = (12.8, 4.8))\n",
        "fig.clf()\n",
        "plt.subplot(1,2,1)\n",
        "plt.scatter(*x.cpu().T,c=y_test,s=1)\n",
        "plt.title(\"test data\")\n",
        "plt.xlabel(r\"$x_1$\")\n",
        "plt.ylabel(r\"$x_2$\")\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.scatter(*z.T, c=y_test, s=1)\n",
        "plt.title(\"z transformed from x\")\n",
        "plt.xlabel(r\"$z_1$\")\n",
        "plt.ylabel(r\"$z_2$\")\n",
        "plt.xlim(-3,3)\n",
        "plt.ylim(-3,3)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLT1dzDpDOq6"
      },
      "outputs": [],
      "source": [
        "# test if inverse of f can transform from z to x\n",
        "# create two scatter plots, the first is sampled from the standard gaussian distribution\n",
        "# the second is the generated z\n",
        "# visualize the class label in the scatter plot\n",
        "\n",
        "###########################   <YOUR CODE>  ############################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###########################   <YOUR CODE>  ############################\n",
        "\n",
        "fig = plt.figure(2, figsize = (12.8, 4.8))\n",
        "fig.clf()\n",
        "plt.subplot(1,2,1)\n",
        "plt.scatter(*z.T,s=1)\n",
        "plt.title(\"Z sampled from normal distribution\")\n",
        "plt.xlabel(r\"$z_1$\")\n",
        "plt.ylabel(r\"$z_2$\")\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.scatter(*X.T,s=1)\n",
        "plt.title(\"X transformed from Z\")\n",
        "plt.xlabel(r\"$x_1$\")\n",
        "plt.ylabel(r\"$x_2$\")\n",
        "# plt.xlim(-3,3)\n",
        "# plt.ylim(-3,3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPiVBiyCDPSl"
      },
      "outputs": [],
      "source": [
        "# create the density heat map using log likelihood on a meshgrid generated by 100 points\n",
        "# from -1.5 to 2.5 in both dimension (10000 points in total)\n",
        "xline = torch.linspace(-1.5,2.5,100)\n",
        "yline = torch.linspace(-1.5,2.5,100)\n",
        "xgrid, ygrid = torch.meshgrid(xline, yline)\n",
        "xyinput = torch.cat([xgrid.reshape(-1, 1), ygrid.reshape(-1, 1)], dim=1)\n",
        "\n",
        "with torch.no_grad():\n",
        "    zgrid = model.cpu().log_prob(xyinput).exp().reshape(100, 100)\n",
        "\n",
        "plt.contourf(xgrid.cpu().numpy(), ygrid.cpu().numpy(), zgrid.cpu().numpy())\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}